# 第11课：线性回归——梯度下降法求解目标函数

导数：在某一点函数的导数，就是变化率，是一个具体的数字

微分：函数在某一点的微分，是个函数，带入数字，就可以得到导数

步长(learning rate)(学习速度)：步长决定了在梯度下降过程中，每一步沿梯度负方向前进的长度

偏导：多元函数的变化率就是偏导

基本求导：

> 1. 常数的导数是零：(c)' = 0；
> 2. x 的 n 次幂的导数是 n 倍的 x 的 n-1 次幂：(xn)′=nxn−1(xn)′=nxn−1；
> 3. 对常数乘以函数求导，结果等于该常数乘以函数的导数：(cf)' = cf'；
> 4. 两个函数 f 和 g 的和的导数为：(f+g)' = f' + g'；
> 5. 两个函数 f 和 g 的积的导数为：(fg)' = f'g + fg'。

线性回归的超参数：

* 步长α
* 迭代次数
* 阈值

梯度下降法是干嘛的？ 

* 梯度下降法是一种以最快的速度找到最优解的方法！ 

梯度下降法流程： 

* 1，初始化theta，w0...wn 
* 2，接着求梯度gradient 
* 3，theta_t+1 = theta_t - grad * learning_rate 
* learning_rate是个超参数，太大容易来回振荡，太小步子太短，需要走很长时间，不管太大还是太小， 都会迭代次数很多，耗时很长 
* 4，等待grad < threshold，迭代停止，收敛，threshold是个超参数 

迭代公式：

* grad_j = (1/m) * (Xj)^Transpose * (X*theta - y) 
* grads = (1/m) * X^Transpose * (X*theta - y) 
* θ=θ−αXT(Xθ−Y)

 梯度下降法求解目标函数：

* ![image-20181222102745145](/Users/zhangjianfeng/Library/Application Support/typora-user-images/image-20181222102745145.png)

* Step1：任意给定a和b的值 a=0;b=0;

* Step2：用梯度下降法求解a和b，注意，α是超参数，a的大小随着梯度的大小而变化

   ![image-20181222102936820](/Users/zhangjianfeng/Library/Application Support/typora-user-images/image-20181222102936820.png)

n维自变量的线性回归模型对应的目标函数：

![image-20181222103326956](/Users/zhangjianfeng/Library/Application Support/typora-user-images/image-20181222103326956.png)

对应的目标函数：![image-20181222103355068](/Users/zhangjianfeng/Library/Application Support/typora-user-images/image-20181222103355068.png)

梯度下降过程中权重的变化:

θ=θ-α*（ðJ(θ）/ðθ)  ,进行迭代



